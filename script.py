from playwright.async_api import async_playwright
import csv
import asyncio
from concurrent.futures import ThreadPoolExecutor

LIST_URL = "https://kariyer.baykartech.com/tr/open-positions/?type=232"
CSV_FILE_ALL = "baykar_tum_ilanlar.csv"
CSV_FILE_FILTERED = "baykar_bilgisayar_muh_staj.csv"

# Aranan anahtar kelimeler (kÃ¼Ã§Ã¼k harfli)
KEYWORDS = [
    "bilgisayar mÃ¼hendisliÄŸi",
    "bilgisayar mÃ¼hendisi",
    "yazÄ±lÄ±m mÃ¼hendisliÄŸi",
    "yazÄ±lÄ±m mÃ¼hendisi",
    "computer engineering",
    "software engineering",
    "bilgisayar mÃ¼h",
    "yazÄ±lÄ±m mÃ¼h"
]

def check_if_computer_engineering(text):
    """Verilen metinde bilgisayar mÃ¼hendisliÄŸi anahtar kelimelerini arar"""
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in KEYWORDS)

async def scrape_job_detail(page, job_url, index, total):
    """Tek bir ilanÄ±n detayÄ±nÄ± asenkron olarak Ã§eker"""
    try:
        await page.goto(job_url, wait_until="networkidle", timeout=20000)
        await asyncio.sleep(0.5)
        
        # Ä°lan baÅŸlÄ±ÄŸÄ±nÄ± al
        title_element = await page.query_selector('h1, h2, .job-title, [class*="title"]')
        title = await title_element.inner_text() if title_element else "BaÅŸlÄ±k bulunamadÄ±"
        title = title.strip()
        
        # SayfanÄ±n tÃ¼m metnini al
        page_text = await page.inner_text('body')
        
        # Bilgisayar mÃ¼hendisliÄŸi kontrolÃ¼
        is_suitable = check_if_computer_engineering(page_text)
        
        status = "âœ…" if is_suitable else "âŒ"
        print(f"[{index}/{total}] {status}", flush=True)
        
        return {
            "ilan_adi": title,
            "ilan_linki": job_url,
            "bilgisayar_muhendisi_basvurabilir": "EVET" if is_suitable else "HAYIR"
        }
    except Exception as e:
        print(f"[{index}/{total}] âš ï¸", flush=True)
        return {
            "ilan_adi": "Hata oluÅŸtu",
            "ilan_linki": job_url,
            "bilgisayar_muhendisi_basvurabilir": "HATA"
        }

async def scrape_page_links(page, page_num):
    """Tek bir sayfa listesinden linkleri asenkron olarak Ã§eker"""
    page_url = f"{LIST_URL}&page={page_num}"
    await page.goto(page_url, wait_until="networkidle", timeout=30000)
    await asyncio.sleep(1)
    
    cards = await page.query_selector_all('a[href*="/acik-pozisyonlar/detay/"]')
    
    job_links = []
    for card in cards:
        href = await card.get_attribute('href')
        if href and '/acik-pozisyonlar/detay/' in href:
            # Sadece 2026-yaz-donemi iÃ§eren ilanlarÄ± al
            if '2026-yaz-donemi' in href.lower():
                full_url = "https://kariyer.baykartech.com" + href if href.startswith('/') else href
                if full_url not in job_links:
                    job_links.append(full_url)
    
    return job_links

async def main():
    print("ğŸš€ TarayÄ±cÄ± baÅŸlatÄ±lÄ±yor (arka planda)...")
    
    async with async_playwright() as p:
        # TarayÄ±cÄ±yÄ± headless modda baÅŸlat (hiÃ§ sekme aÃ§Ä±lmaz)
        browser = await p.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled']
        )
        
        # Context oluÅŸtur (gerÃ§ek browser gibi gÃ¶rÃ¼nmek iÃ§in)
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080}
        )
        
        # Paralel link toplama iÃ§in 3 sayfa aÃ§
        print("ğŸ“‹ TÃ¼m sayfalardan ilan linkleri toplanÄ±yor...")
        
        pages_for_listing = [await context.new_page() for _ in range(3)]
        
        all_job_links = []
        tasks = []
        
        # 10 sayfayÄ± 3 paralel page ile topla
        for page_num in range(1, 11):
            page_index = (page_num - 1) % 3
            task = scrape_page_links(pages_for_listing[page_index], page_num)
            tasks.append((page_num, task))
        
        # Her 3 gÃ¶revde bir batch iÅŸle
        for i in range(0, len(tasks), 3):
            batch = tasks[i:i+3]
            results = await asyncio.gather(*[task for _, task in batch])
            
            for j, (page_num, _) in enumerate(batch):
                links = results[j]
                all_job_links.extend(links)
                print(f"   ğŸ“„ Sayfa {page_num}/10 â†’ {len(links)} ilan bulundu")
        
        # Listing page'leri kapat
        for page in pages_for_listing:
            await page.close()
        
        # Duplicate linkleri temizle
        job_links = list(dict.fromkeys(all_job_links))
        
        print(f"\nğŸ“Œ Toplam {len(job_links)} ilan bulundu")
        print("ğŸ” Ä°lanlar kontrol ediliyor (5 paralel)...\n")
        
        # Detay sayfalarÄ± iÃ§in 5 paralel page aÃ§
        pages_for_details = [await context.new_page() for _ in range(5)]
        
        results = []
        
        # Her 5 ilanÄ± aynÄ± anda iÅŸle
        for i in range(0, len(job_links), 5):
            batch_links = job_links[i:i+5]
            batch_tasks = []
            
            for j, job_url in enumerate(batch_links):
                page_index = j % 5
                task = scrape_job_detail(
                    pages_for_details[page_index],
                    job_url,
                    i + j + 1,
                    len(job_links)
                )
                batch_tasks.append(task)
            
            batch_results = await asyncio.gather(*batch_tasks)
            results.extend(batch_results)
        
        # Detay page'leri kapat
        for page in pages_for_details:
            await page.close()
        
        await context.close()
        await browser.close()
    
    # CSV dosyalarÄ±na kaydet
    print(f"\n\nğŸ’¾ SonuÃ§lar kaydediliyor...")
    
    # 1. TÃœM Ä°LANLARI KAYDET
    with open(CSV_FILE_ALL, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["ilan_adi", "ilan_linki", "bilgisayar_muhendisi_basvurabilir"]
        )
        writer.writeheader()
        writer.writerows(results)
    
    # 2. SADECE BÄ°LGÄ°SAYAR MÃœHENDÄ°SLÄ°ÄÄ° Ä°Ã‡Ä°N UYGUN OLANLARI KAYDET
    filtered_results = [r for r in results if r['bilgisayar_muhendisi_basvurabilir'] == 'EVET']
    
    with open(CSV_FILE_FILTERED, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["ilan_adi", "ilan_linki", "bilgisayar_muhendisi_basvurabilir"]
        )
        writer.writeheader()
        writer.writerows(filtered_results)
    
    print(f"\nğŸ“ Ä°ÅŸlem tamamlandÄ±!")
    print(f"â¡ TÃ¼m ilanlar: '{CSV_FILE_ALL}' dosyasÄ±na kaydedildi")
    print(f"â¡ Bilgisayar MÃ¼h. ilanlarÄ±: '{CSV_FILE_FILTERED}' dosyasÄ±na kaydedildi")
    print(f"\nğŸ“Š Ã–ZET:")
    print(f"   â€¢ Toplam ilan: {len(results)}")
    print(f"   â€¢ Bilgisayar MÃ¼h. iÃ§in uygun: {len(filtered_results)} ilan")
    print(f"   â€¢ Uygun olmayan: {len(results) - len(filtered_results)} ilan")

if __name__ == "__main__":
    asyncio.run(main())
